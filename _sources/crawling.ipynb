{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Crawling Data From Web PTA.Trunojoyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk melakukan proses crawling data pada Website, kita membutuhkan Library Scrapy. maka dari itu kita install dulu dengan cara ketik di CMD :\n",
    "\n",
    "\"pip install Scrapy\"\n",
    "\n",
    "Lalu kita import library tersebut dengan code dibawah ini:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class Url(scrapy.Spider):\n",
    "    name = \"url\"\n",
    "    start_urls = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        url = 'https://pta.trunojoyo.ac.id/c_search/byprod/10/'\n",
    "        for page in range(1,13):\n",
    "            self.start_urls.append(url + str(page))\n",
    "        \n",
    "    def parse(self, response):\n",
    "        for page in range(1,6):\n",
    "            for url in response.css('#content_journal > ul'):\n",
    "                yield {\n",
    "                    'url' : url.css('li:nth-child('+str(page)+') > div:nth-child(3) > a ::attr(href)').extract()\n",
    "                } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code diatas berfungsi untuk mengambil link tiap berita yang ingin kita ambil datanya.\n",
    "Untuk menjalankan code diatas, kita harus save code tersebut dengan format .py agar dapat di run. setelah itu kita dapat menjalankan codenya dengan perintah \"scrapy runspider -namafile.py -o -namafile.json\".\n",
    "untuk namafile.json adalah hasil run dari code diatas yang berisikan link dari berita yang akan kita ambil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "\n",
    "class Pta(scrapy.Spider):\n",
    "    name = \"pta\"\n",
    "    file_json = open(\"url.json\")\n",
    "    start_urls = json.loads(file_json.read())\n",
    "    urls = []\n",
    "\n",
    "    for i in range(len(start_urls)):\n",
    "        b = start_urls[i]['url'][0]\n",
    "        urls.append(b)\n",
    "    \n",
    "    def start_requests(self):\n",
    "        for url in self.urls:\n",
    "            yield scrapy.Request(url = url, callback = self.parse)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        # print(response.url)\n",
    "\n",
    "        for jurnal in response.css('#content_journal > ul > li'):\n",
    "            yield {\n",
    "                'Judul':jurnal.css('div:nth-child(2) > a::text').get(),\n",
    "                'Penulis':jurnal.css('div:nth-child(2) > div:nth-child(2) > span::text').get()[10:],\n",
    "                'Dosbing_1':jurnal.css('div:nth-child(2) > div:nth-child(3) > span::text').get()[21:],\n",
    "                'Dosbing_2':jurnal.css('div:nth-child(2) > div:nth-child(4) > span::text').get()[22:],\n",
    "                'Abstrak_indo':jurnal.css('div:nth-child(4) > div:nth-child(2) > p::text').get(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code diatas berfungsi untuk mengambil bagian yang perlu kita ambil pada tiap halaman PTA. sebelum itu kita harus import library Json terlebih dahulu, dikarenakan sebelumnya untuk link halaman PTA kita jadikan format .json\n",
    "\n",
    "Pada code ini yang saya ambil adalah bagian Judul, Penulis, Dosen pembimbing 1, Dosen Pembimbing 2, dan Abstrak bahasa indonesianya dengan cara menginspect bagian yang akan kita ambil. setelah data di ambil, data tersebut kita jadikan format .csv untuk pemrosesan selanjutnya dengan cara run di terminal :\n",
    "\n",
    "\"scrapy runspider namafile.py -o namafile.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1b15f5df67a702be8f228f6d0b6061fe0c39795279c81537b9d6008feba9a7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
